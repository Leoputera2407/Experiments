INFO:lightning:
    | Name                                                                  | Type                       | Params
-----------------------------------------------------------------------------------------------------------------
0   | model                                                                 | T5ForConditionalGeneration | 222 M 
1   | model.shared                                                          | Embedding                  | 24 M  
2   | model.encoder                                                         | T5Stack                    | 109 M 
3   | model.encoder.block                                                   | ModuleList                 | 84 M  
4   | model.encoder.block.0                                                 | T5Block                    | 7 M   
5   | model.encoder.block.0.layer                                           | ModuleList                 | 7 M   
6   | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   
7   | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   
8   | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K 
9   | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K 
10  | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K 
11  | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K 
12  | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   
13  | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   
14  | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     
15  | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 4 M   
16  | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
17  | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
18  | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
19  | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
20  | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   
21  | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     
22  | model.encoder.block.1                                                 | T5Block                    | 7 M   
23  | model.encoder.block.1.layer                                           | ModuleList                 | 7 M   
24  | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   
25  | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   
26  | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K 
27  | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K 
28  | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K 
29  | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K 
30  | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   
31  | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     
32  | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 4 M   
33  | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
34  | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
35  | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
36  | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
37  | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   
38  | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     
39  | model.encoder.block.2                                                 | T5Block                    | 7 M   
40  | model.encoder.block.2.layer                                           | ModuleList                 | 7 M   
41  | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   
42  | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   
43  | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K 
44  | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K 
45  | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K 
46  | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K 
47  | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   
48  | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     
49  | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 4 M   
50  | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
51  | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
52  | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
53  | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
54  | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   
55  | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     
56  | model.encoder.block.3                                                 | T5Block                    | 7 M   
57  | model.encoder.block.3.layer                                           | ModuleList                 | 7 M   
58  | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   
59  | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   
60  | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K 
61  | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K 
62  | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K 
63  | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K 
64  | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   
65  | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     
66  | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 4 M   
67  | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
68  | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
69  | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
70  | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
71  | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   
72  | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     
73  | model.encoder.block.4                                                 | T5Block                    | 7 M   
74  | model.encoder.block.4.layer                                           | ModuleList                 | 7 M   
75  | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   
76  | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   
77  | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K 
78  | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K 
79  | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K 
80  | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K 
81  | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   
82  | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     
83  | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 4 M   
84  | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
85  | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
86  | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
87  | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
88  | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   
89  | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     
90  | model.encoder.block.5                                                 | T5Block                    | 7 M   
91  | model.encoder.block.5.layer                                           | ModuleList                 | 7 M   
92  | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   
93  | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   
94  | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K 
95  | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K 
96  | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K 
97  | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K 
98  | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   
99  | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     
100 | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 4 M   
101 | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
102 | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
103 | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
104 | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
105 | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   
106 | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     
107 | model.encoder.block.6                                                 | T5Block                    | 7 M   
108 | model.encoder.block.6.layer                                           | ModuleList                 | 7 M   
109 | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   
110 | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   
111 | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K 
112 | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K 
113 | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K 
114 | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K 
115 | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   
116 | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     
117 | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 4 M   
118 | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
119 | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
120 | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
121 | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
122 | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   
123 | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     
124 | model.encoder.block.7                                                 | T5Block                    | 7 M   
125 | model.encoder.block.7.layer                                           | ModuleList                 | 7 M   
126 | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   
127 | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   
128 | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K 
129 | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K 
130 | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K 
131 | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K 
132 | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   
133 | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     
134 | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 4 M   
135 | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
136 | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
137 | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
138 | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
139 | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   
140 | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     
141 | model.encoder.block.8                                                 | T5Block                    | 7 M   
142 | model.encoder.block.8.layer                                           | ModuleList                 | 7 M   
143 | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   
144 | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   
145 | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K 
146 | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K 
147 | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K 
148 | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K 
149 | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   
150 | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     
151 | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 4 M   
152 | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
153 | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
154 | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
155 | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
156 | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   
157 | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     
158 | model.encoder.block.9                                                 | T5Block                    | 7 M   
159 | model.encoder.block.9.layer                                           | ModuleList                 | 7 M   
160 | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   
161 | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   
162 | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K 
163 | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K 
164 | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K 
165 | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K 
166 | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   
167 | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     
168 | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 4 M   
169 | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   
170 | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   
171 | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   
172 | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     
173 | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   
174 | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     
175 | model.encoder.block.10                                                | T5Block                    | 7 M   
176 | model.encoder.block.10.layer                                          | ModuleList                 | 7 M   
177 | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   
178 | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   
179 | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K 
180 | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K 
181 | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K 
182 | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K 
183 | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   
184 | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     
185 | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 4 M   
186 | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   
187 | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   
188 | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   
189 | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     
190 | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   
191 | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     
192 | model.encoder.block.11                                                | T5Block                    | 7 M   
193 | model.encoder.block.11.layer                                          | ModuleList                 | 7 M   
194 | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   
195 | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   
196 | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K 
197 | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K 
198 | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K 
199 | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K 
200 | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   
201 | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     
202 | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 4 M   
203 | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   
204 | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   
205 | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   
206 | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     
207 | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   
208 | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     
209 | model.encoder.final_layer_norm                                        | T5LayerNorm                | 768   
210 | model.encoder.dropout                                                 | Dropout                    | 0     
211 | model.decoder                                                         | T5Stack                    | 137 M 
212 | model.decoder.block                                                   | ModuleList                 | 113 M 
213 | model.decoder.block.0                                                 | T5Block                    | 9 M   
214 | model.decoder.block.0.layer                                           | ModuleList                 | 9 M   
215 | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   
216 | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   
217 | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K 
218 | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K 
219 | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K 
220 | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K 
221 | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   
222 | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   
223 | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     
224 | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 2 M   
225 | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 2 M   
226 | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
227 | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
228 | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
229 | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
230 | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 384   
231 | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   
232 | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     
233 | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 4 M   
234 | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
235 | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
236 | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
237 | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
238 | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 768   
239 | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     
240 | model.decoder.block.1                                                 | T5Block                    | 9 M   
241 | model.decoder.block.1.layer                                           | ModuleList                 | 9 M   
242 | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   
243 | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   
244 | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K 
245 | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K 
246 | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K 
247 | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K 
248 | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   
249 | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     
250 | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 2 M   
251 | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 2 M   
252 | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
253 | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
254 | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
255 | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
256 | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   
257 | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     
258 | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 4 M   
259 | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
260 | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
261 | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
262 | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
263 | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 768   
264 | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     
265 | model.decoder.block.2                                                 | T5Block                    | 9 M   
266 | model.decoder.block.2.layer                                           | ModuleList                 | 9 M   
267 | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   
268 | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   
269 | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K 
270 | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K 
271 | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K 
272 | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K 
273 | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   
274 | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     
275 | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 2 M   
276 | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 2 M   
277 | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
278 | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
279 | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
280 | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
281 | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   
282 | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     
283 | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 4 M   
284 | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
285 | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
286 | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
287 | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
288 | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 768   
289 | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     
290 | model.decoder.block.3                                                 | T5Block                    | 9 M   
291 | model.decoder.block.3.layer                                           | ModuleList                 | 9 M   
292 | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   
293 | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   
294 | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K 
295 | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K 
296 | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K 
297 | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K 
298 | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   
299 | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     
300 | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 2 M   
301 | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 2 M   
302 | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
303 | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
304 | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
305 | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
306 | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   
307 | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     
308 | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 4 M   
309 | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
310 | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
311 | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
312 | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
313 | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 768   
314 | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     
315 | model.decoder.block.4                                                 | T5Block                    | 9 M   
316 | model.decoder.block.4.layer                                           | ModuleList                 | 9 M   
317 | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   
318 | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   
319 | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K 
320 | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K 
321 | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K 
322 | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K 
323 | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   
324 | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     
325 | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 2 M   
326 | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 2 M   
327 | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
328 | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
329 | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
330 | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
331 | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   
332 | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     
333 | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 4 M   
334 | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
335 | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
336 | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
337 | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
338 | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 768   
339 | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     
340 | model.decoder.block.5                                                 | T5Block                    | 9 M   
341 | model.decoder.block.5.layer                                           | ModuleList                 | 9 M   
342 | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   
343 | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   
344 | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K 
345 | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K 
346 | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K 
347 | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K 
348 | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   
349 | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     
350 | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 2 M   
351 | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 2 M   
352 | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
353 | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
354 | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
355 | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
356 | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   
357 | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     
358 | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 4 M   
359 | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
360 | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
361 | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
362 | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
363 | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 768   
364 | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     
365 | model.decoder.block.6                                                 | T5Block                    | 9 M   
366 | model.decoder.block.6.layer                                           | ModuleList                 | 9 M   
367 | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   
368 | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   
369 | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K 
370 | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K 
371 | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K 
372 | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K 
373 | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   
374 | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     
375 | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 2 M   
376 | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 2 M   
377 | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
378 | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
379 | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
380 | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
381 | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   
382 | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     
383 | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 4 M   
384 | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
385 | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
386 | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
387 | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
388 | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 768   
389 | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     
390 | model.decoder.block.7                                                 | T5Block                    | 9 M   
391 | model.decoder.block.7.layer                                           | ModuleList                 | 9 M   
392 | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   
393 | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   
394 | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K 
395 | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K 
396 | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K 
397 | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K 
398 | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   
399 | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     
400 | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 2 M   
401 | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 2 M   
402 | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
403 | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
404 | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
405 | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
406 | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   
407 | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     
408 | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 4 M   
409 | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
410 | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
411 | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
412 | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
413 | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 768   
414 | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     
415 | model.decoder.block.8                                                 | T5Block                    | 9 M   
416 | model.decoder.block.8.layer                                           | ModuleList                 | 9 M   
417 | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   
418 | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   
419 | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K 
420 | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K 
421 | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K 
422 | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K 
423 | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   
424 | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     
425 | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 2 M   
426 | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 2 M   
427 | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
428 | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
429 | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
430 | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
431 | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   
432 | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     
433 | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 4 M   
434 | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
435 | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
436 | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
437 | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
438 | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 768   
439 | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     
440 | model.decoder.block.9                                                 | T5Block                    | 9 M   
441 | model.decoder.block.9.layer                                           | ModuleList                 | 9 M   
442 | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   
443 | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   
444 | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K 
445 | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K 
446 | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K 
447 | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K 
448 | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   
449 | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     
450 | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 2 M   
451 | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 2 M   
452 | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 589 K 
453 | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 589 K 
454 | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 589 K 
455 | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 589 K 
456 | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   
457 | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     
458 | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 4 M   
459 | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   
460 | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   
461 | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   
462 | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     
463 | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 768   
464 | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     
465 | model.decoder.block.10                                                | T5Block                    | 9 M   
466 | model.decoder.block.10.layer                                          | ModuleList                 | 9 M   
467 | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   
468 | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   
469 | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K 
470 | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K 
471 | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K 
472 | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K 
473 | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   
474 | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     
475 | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 2 M   
476 | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 2 M   
477 | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 589 K 
478 | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 589 K 
479 | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 589 K 
480 | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 589 K 
481 | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   
482 | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     
483 | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 4 M   
484 | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   
485 | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   
486 | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   
487 | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     
488 | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 768   
489 | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     
490 | model.decoder.block.11                                                | T5Block                    | 9 M   
491 | model.decoder.block.11.layer                                          | ModuleList                 | 9 M   
492 | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   
493 | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   
494 | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K 
495 | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K 
496 | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K 
497 | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K 
498 | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   
499 | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     
500 | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 2 M   
501 | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 2 M   
502 | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 589 K 
503 | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 589 K 
504 | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 589 K 
505 | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 589 K 
506 | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   
507 | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     
508 | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 4 M   
509 | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   
510 | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   
511 | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   
512 | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     
513 | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 768   
514 | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     
515 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 768   
516 | model.decoder.dropout                                                 | Dropout                    | 0     
517 | model.lm_head                                                         | Linear                     | 24 M  
Traceback (most recent call last):
  File "train.py", line 328, in <module>
    trainer.fit(model)
  File "/Users/halim/.virtualenvs/para/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 793, in fit
    self.run_pretrain_routine(model)
  File "/Users/halim/.virtualenvs/para/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 890, in run_pretrain_routine
    self.reset_val_dataloader(ref_model)
  File "/Users/halim/.virtualenvs/para/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py", line 256, in reset_val_dataloader
    self._reset_eval_dataloader(model, 'val')
  File "/Users/halim/.virtualenvs/para/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py", line 209, in _reset_eval_dataloader
    dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader'))
  File "/Users/halim/.virtualenvs/para/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py", line 277, in request_dataloader
    dataloader = dataloader_fx()
  File "train.py", line 156, in val_dataloader
    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path="Quora_Paraphrasing_val", args=self.hparams)
  File "train.py", line 318, in get_dataset
    return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)
  File "train.py", line 224, in __init__
    with open(self.path, 'r', encoding='utf8') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'data/Quora_Paraphrasing_val.txt'
