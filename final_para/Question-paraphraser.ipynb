{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0\n",
    "!pip install transformers==2.9.0\n",
    "!pip install pytorch_lightning==0.7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with any question as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/4411\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "from rouge import Rouge \n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import re\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Processing the utterances now...\")\n",
    "\n",
    "def load_slots_dict()-> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    RETURNS\n",
    "    -------\n",
    "        (dict of string): a list of all slots that we've encountered before\n",
    "    \"\"\"\n",
    "    try:\n",
    "        slots_dict ={}\n",
    "        with open('known_slots.txt', 'r', encoding='utf8') as f:\n",
    "            for row in f:\n",
    "                SPLIT_TOKEN = ' <equals> '\n",
    "                slot, placeholder  = row.split(SPLIT_TOKEN)\n",
    "                slots_dict[slot] = placeholder.rstrip(\"\\n\")\n",
    "        return slots_dict\n",
    "    except:\n",
    "        logger.warning('known_slots.txt not found. Please specify the instance of the slots!')\n",
    "        return None\n",
    "    \n",
    "raw_sent= []\n",
    "with open('sample_utterances.txt', 'r', encoding='utf8') as f:\n",
    "    for sent in f:\n",
    "        raw_sent.append(sent)\n",
    "        \n",
    "slots = set()\n",
    "for sent in raw_sent:\n",
    "    get_slots = re.findall(r'{(.*?)}', sent)\n",
    "    slots.update(get_slots)\n",
    "\n",
    "known_slot_dict = load_slots_dict()\n",
    "\n",
    "#If can't find load_slots_dict\n",
    "if not known_slot_dict:\n",
    "    for slot in slots:\n",
    "        known_slot_dict={}\n",
    "        file1 = open(\"known_slots.txt\", \"a\")\n",
    "        placeholder = input(\"Teach me an instance of \" + slot + \":\")\n",
    "        known_slot_dict[slot] = str(placeholder)\n",
    "        file1.write(str(slot) + ' <equals> ' + str(placeholder) + '\\n')\n",
    "        file1.close()\n",
    "\n",
    "#Unpack slots that we already know into a set\n",
    "slots_known = {*known_slot_dict.keys()}\n",
    "\n",
    "#If there exists some unidentifiable slots\n",
    "if not slots.issubset(slots_known):\n",
    "    slots_seen = slots.intersection(slots_known)\n",
    "    unknown_slots = slots.difference(slots_seen)\n",
    "    for slot in unknown_slots:\n",
    "        file1 = open(\"known_slots.txt\", \"a\")\n",
    "        placeholder = input(\"Teach me an instance of \" + slot + \":\")\n",
    "        known_slot_dict[slot] = str(placeholder)\n",
    "        file1.write(str(slot) + ' <equals> ' + str(placeholder) + '\\n')\n",
    "        file1.close()\n",
    "        \n",
    "        \n",
    "def multiple_replace(known_slots_dict: Dict[str,str], text: str):\n",
    "    \"\"\"\n",
    "    Function takes a dictionary of known_slots (slots: placeholder) and replace everything in the text \n",
    "    that has the {slots} pattern with its placeholder.\n",
    "    Reference\n",
    "    ---------\n",
    "    https://stackoverflow.com/questions/15175142/how-can-i-do-multiple-substitutions-using-regex-in-python\n",
    "    \n",
    "    ARGS\n",
    "    ----\n",
    "        known_slots_dict: dictionary of known_slots (slots: placeholder)\n",
    "        text: raw sentence (with slots) to be replaced\n",
    "    \"\"\" \n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"{(%s)}\" % \"|\".join(map(re.escape, known_slots_dict.keys())))\n",
    "    \n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: known_slots_dict[mo.string[mo.start()+1:mo.end()-1]], text) \n",
    "\n",
    "\n",
    "# Replace slots in strings\n",
    "CLEAN_SEN = []\n",
    "for sent in raw_sent:\n",
    "    CLEAN_SEN.append(multiple_replace(known_slot_dict,sent))\n",
    "    \n",
    "logger.info(\"Finished cleaning the utterances...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Setupping up models to generate paraphrases...\")\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5_paraphrase')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Generating paraphrases on {device}.Recommend GPU for speed, of course!\", device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "logger.info(\"Preparing USE Embedding...Might have to download...\")\n",
    "#USE Embedder\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder-large/4\"\n",
    "embed = hub.load(url)\n",
    "logger.info(\"Done preparing USE Embedding\")\n",
    "\n",
    "\n",
    "#Beam-search config\n",
    "MAX_SEQ_LEN = 256\n",
    "# Number of Paraphrases you want to generate\n",
    "NB_GENERATED = 100\n",
    "# Top N to keep\n",
    "TOP_TO_KEEP = 10\n",
    "#TOP_P (values bwn 0-1): threshold to keep token for nucleus sampling\n",
    "TOP_P = 0.90\n",
    "#TODO: Do a sweep in the right top_k/top_p value.\n",
    "\n",
    "logger.info(\"Generating Paraphrase now\")\n",
    "#Put in proper format\n",
    "INPUT_SEN = []\n",
    "for sent in CLEAN_SEN:\n",
    "    INPUT_SEN.append(\"paraphrase: \" + sent + \" </s>\")\n",
    "\n",
    "def get_n_best_para(input_sentence: str, paraphrases: List[str], top_n: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    RETURNS\n",
    "    -------\n",
    "        (list of strings): top n paraphrases that are most semantically similar (using USE embeddings) and most\n",
    "            different structurally (using L-Rouge) to the input_sentence\n",
    "    \"\"\" \n",
    "    rouge = Rouge() \n",
    "    rouge_scrs = [1- rouge.get_scores(input_sentence, para)[0]['rouge-l']['f'] for para in paraphrases]\n",
    "    \n",
    "    #NOTE: Measure similarity using inner-product on USE embedding.\n",
    "    #enc_input_sentence, *enc_paraphrases = self.embed([input_sentence] + paraphrases)\n",
    "    enc_input_sentence = embed([input_sentence])\n",
    "    enc_paraphrases = embed(paraphrases)\n",
    "    MEANING_DIV_RATIO = 0.65\n",
    "    scored_paraphrases = [\n",
    "        (paraphrase, np.inner(enc_input_sentence['outputs'].numpy(), enc_paraphrase)[0] * MEANING_DIV_RATIO + score * (1-MEANING_DIV_RATIO))\n",
    "        for (paraphrase, enc_paraphrase, score) in zip(paraphrases, enc_paraphrases['outputs'].numpy(),rouge_scrs)\n",
    "    ]\n",
    "    #Sort on meaning, then diversity\n",
    "    top_n_paraphrases = sorted(scored_paraphrases, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [x[0] for x in top_n_paraphrases]\n",
    "\n",
    "for idx, text in tqdm(enumerate(INPUT_SEN)):\n",
    "    logger.info(\"Generating for utterance {}\", idx+1)\n",
    "    encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # https://huggingface.co/transformers/model_doc/t5.html?highlight=generate#overview\n",
    "    # https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.PreTrainedModel.generate\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids=input_ids, attention_mask=attention_masks,\n",
    "        do_sample=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        early_stopping=True,\n",
    "        top_k=100,\n",
    "        top_p=TOP_P,\n",
    "        num_return_sequences=NB_GENERATED\n",
    "    )\n",
    "    paraphrases =[]\n",
    "    for beam_output in beam_outputs:\n",
    "        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "        if sent.lower() != text.lower() and sent not in paraphrases:\n",
    "            paraphrases.append(sent)\n",
    "            \n",
    "    top_para = get_n_best_para(CLEAN_SEN[idx], paraphrases, TOP_TO_KEEP)\n",
    "    print(\"INPUT SENTENCE :\", CLEAN_SEN[idx])\n",
    "    print(\"PARAPRHASES :\")\n",
    "    for i, paraphrase in enumerate(top_para):\n",
    "        print(\"nÂ°%d : %s\" % (i, paraphrase))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: mjehgs9a\n",
      "Sweep URL: https://app.wandb.ai/leoputera2407/project-name/sweeps/mjehgs9a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/3l/0t74styx5wz0y5tkfgsvhycwfc38wl/T/tfhub_modules to cache modules.\n",
      "INFO:wandb.wandb_agent:Running runs: []\n",
      "INFO:wandb.wandb_agent:Agent received command: run\n",
      "INFO:wandb.wandb_agent:Agent starting run with config:\n",
      "\tnum_return_sequences: 10\n",
      "\ttop_k: 50\n",
      "\ttop_p: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: jvi1tfnz with config:\n",
      "\tnum_return_sequences: 10\n",
      "\ttop_k: 50\n",
      "\ttop_p: 0.5\n",
      "wandb: Agent Started Run: jvi1tfnz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='hyperparameter-sweeps-comparison' passed to wandb.init when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/leoputera2407/project-name\" target=\"_blank\">https://app.wandb.ai/leoputera2407/project-name</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/leoputera2407/project-name/sweeps/mjehgs9a\" target=\"_blank\">https://app.wandb.ai/leoputera2407/project-name/sweeps/mjehgs9a</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/leoputera2407/project-name/runs/jvi1tfnz\" target=\"_blank\">https://app.wandb.ai/leoputera2407/project-name/runs/jvi1tfnz</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.wandb_agent:Running runs: ['jvi1tfnz']\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "from rouge import Rouge \n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"My Sweep\",\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\":{\n",
    "        \"top_k\":{\n",
    "            \"values\" : [50,75,100, 125, 150,175]\n",
    "        },\n",
    "        \"top_p\":{\n",
    "            \"values\": [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        },\n",
    "        \"num_return_sequences\":{\n",
    "            \"values\": [10, 20, 40, 60, 80, 100, 300, 500, 800]\n",
    "        },\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='project-name')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5_paraphrase')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder-large/4\"\n",
    "embed = hub.load(url)\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train():\n",
    "    hyperparam_defaults = dict(\n",
    "        top_k = 40,\n",
    "        top_p = 0.4,\n",
    "        num_return_sequences = 5,\n",
    "    )\n",
    "    wandb.init(project='hyperparameter-sweeps-comparison', config=hyperparam_defaults)\n",
    "    config = wandb.config\n",
    "    set_seed(42)\n",
    "    MAX_SEQ_LEN = 256\n",
    "    INPUT_SEN = \"What is the sales of the game on the platform this year?\"\n",
    "    text = \"paraphrase: \" + INPUT_SEN + \" </s>\"\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "    \n",
    "    beam_outputs = model.generate(\n",
    "        input_ids=input_ids, attention_mask=attention_masks,\n",
    "        do_sample=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        early_stopping=True,\n",
    "        top_k= config.top_k,\n",
    "        top_p= config.top_p,\n",
    "        num_return_sequences= config.num_return_sequences\n",
    "    )\n",
    "    paraphrases =[]\n",
    "    for beam_output in beam_outputs:\n",
    "        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "        if sent.lower() != text.lower() and sent not in paraphrases:\n",
    "            paraphrases.append(sent)\n",
    "            \n",
    "    enc_input_sentence = embed([INPUT_SEN])\n",
    "    enc_paraphrases = embed(paraphrases)\n",
    "    \n",
    "    avg_USE = np.mean([np.inner(enc_input_sentence['outputs'].numpy(),\n",
    "                                enc_paraphrase)[0] for enc_paraphrase in enc_paraphrases['outputs'].numpy()])\n",
    "    avg_rouge = np.mean([1- rouge.get_scores(input_sentence, para)[0]['rouge-l']['f'] for para in paraphrases])\n",
    "    metrics = {'avg_embed score': avg_USE, 'avg_rouge': avg_rouge}\n",
    "    wandb.log(metrics)\n",
    "\n",
    "wandb.agent(sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para",
   "language": "python",
   "name": "para"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
