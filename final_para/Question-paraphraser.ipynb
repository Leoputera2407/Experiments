{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0\n",
    "!pip install transformers==2.9.0\n",
    "!pip install pytorch_lightning==0.7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with any question as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n",
      "91.46912192286942\n",
      "INPUT SENTENCE : What is the ARIMA forecasted revenue for the next 4 months?\n",
      "PARAPRHASES :\n",
      "n°0 : What is ARIMA forecasted revenue for the next 4 months?\n",
      "n°1 : What is the ARIMA projected revenue for the next 4 months?\n",
      "n°2 : What is the ARIMA forecast revenue for the next 4 months?\n",
      "n°3 : What is the projected ARIMA revenue for the next 4 months?\n",
      "n°4 : What is the forecast ARIMA revenue for the next 4 months?\n",
      "n°5 : What is the ARIMA estimated revenue for the next 4 months?\n",
      "n°6 : What is ARIMA projected revenue for the next 4 months?\n",
      "n°7 : What is ARIMA forecast revenue for the next 4 months?\n",
      "n°8 : What is the revenue forecast of ARIMA for the next 4 months?\n",
      "n°9 : What is ARIMA revenue forecast for the next 4 months?\n",
      "INPUT SENTENCE : What is the ARIMA forecasted revenue for the next 4 months?\n",
      "PARAPRHASES :\n",
      "n°0 : What is the projected ARIMA revenue for the next 4 months?\n",
      "n°1 : How will ARIMA's revenues be for the next 4 months?\n",
      "n°2 : What is ARIMA forecasted revenue for the next 4 months?\n",
      "n°3 : What is ARIMA's forecast revenue for the next 4 months?\n",
      "n°4 : How is ARIMA's forecast revenue for the next 4 months?\n",
      "n°5 : What is ARIMA's projected revenue for the next four months?\n",
      "n°6 : What will ARIMA's projected revenue be in 3 to 4 months?\n",
      "n°7 : What is ARIMA's estimated revenue for 2016-17?\n",
      "n°8 : What is ARIMA revenue forecast?\n",
      "n°9 : What is ARIMA forecasted revenue for the next 4 months?\n",
      "n°10 : What is the ARIMA projected revenue for the next 4 months?\n",
      "n°11 : What is ARIMA forecast revenue for the next 4 months?\n",
      "n°12 : What is the ARIMA forecast revenue for the next 4 months?\n",
      "n°13 : How much is ARIMA's forecast revenue for the next 4 months?\n",
      "n°14 : How is the ARIMA's revenue forecast for the next 4 months?\n",
      "n°15 : What is the ARIMA estimated revenue for the next 4 months?\n",
      "n°16 : What is ARIMA projected revenue for the next 4 months?\n",
      "n°17 : What is the ARIMA forecast revenue for the next 4 months?\n",
      "n°18 : What is ARIMA forecast revenue for the next 4 months?\n",
      "n°19 : How is ARIMA predicted revenue for the next 4 months?\n",
      "n°20 : What will be ARIMA's revenue forecast in the next 4 months?\n",
      "n°21 : What is the projected ARIMA revenue for the next 4 months?\n",
      "n°22 : What is the ARIMA projected revenue in 2018?\n",
      "n°23 : What is the ARIMA forecast revenue for the next 4 months?\n",
      "n°24 : What is the forecast ARIMA revenue for the next 4 months?\n",
      "n°25 : What is the revenue forecast of ARIMA for the next 4 months?\n",
      "n°26 : What is ARIMA's forecasted revenue for the next four months?\n",
      "n°27 : What is ARIMA revenue forecast for the next 4 months?\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/issues/4411\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "from rouge import Rouge \n",
    "from typing import List\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "\n",
    "\"\"\"\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\"\"\"\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5_paraphrase')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)\n",
    "\n",
    "#USE Embedder\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder-large/4\"\n",
    "embed = hub.load(url)\n",
    "\n",
    "INPUT_SEN = \"What is the ARIMA forecasted revenue for the next 4 months?\"\n",
    "\n",
    "#Beam-search config\n",
    "MAX_SEQ_LEN = 256\n",
    "# Number of Paraphrases you want to generate\n",
    "NB_GENERATED = 30\n",
    "# Top N to keep\n",
    "TOP_TO_KEEP = 10\n",
    "#TOP_P (values bwn 0-1): threshold to keep token for nucleus sampling\n",
    "TOP_P = 0.90\n",
    "#TODO: Do a sweep in the right top_k/top_p value.\n",
    "\n",
    "\n",
    "#Put in proper format\n",
    "text =  \"paraphrase: \" + INPUT_SEN + \" </s>\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# https://huggingface.co/transformers/model_doc/t5.html?highlight=generate#overview\n",
    "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.PreTrainedModel.generate\n",
    "beam_outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    early_stopping=True,\n",
    "    top_k=100,\n",
    "    top_p=TOP_P,\n",
    "    num_return_sequences=NB_GENERATED\n",
    ")\n",
    "\n",
    "\n",
    "paraphrases =[]\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
    "        paraphrases.append(sent)\n",
    "        \n",
    "def get_n_best_para(input_sentence: str, paraphrases: List[str], top_n: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "        (list of strings): top n paraphrases that are most semantically similar (using USE embeddings) and most\n",
    "            different structurally (using L-Rouge) to the input_sentence\n",
    "    \"\"\"\n",
    "    #Remove duplicate sentences\n",
    "    paraphrases = list(set(paraphrases))\n",
    "    if len(paraphrases) < top_n:\n",
    "        top_n = len(paraphrases)\n",
    "        \n",
    "    rouge = Rouge() \n",
    "    rouge_scrs = [1- rouge.get_scores(input_sentence, para)[0]['rouge-l']['f'] for para in paraphrases]\n",
    "    \n",
    "    ting = [[para] for para in paraphrases]\n",
    "    bleu = sacrebleu.corpus_bleu(input_sentence , ting)\n",
    "    print(bleu.score)\n",
    "    \n",
    "    #NOTE: Measure similarity using inner-product on USE embedding.\n",
    "    #enc_input_sentence, *enc_paraphrases = self.embed([input_sentence] + paraphrases)\n",
    "    enc_input_sentence = embed([input_sentence])\n",
    "    enc_paraphrases = embed(paraphrases)\n",
    "    scored_paraphrases = [\n",
    "        (paraphrase, np.inner(enc_input_sentence['outputs'].numpy(), enc_paraphrase), score)\n",
    "        for (paraphrase, enc_paraphrase, score) in zip(paraphrases, enc_paraphrases['outputs'].numpy(),rouge_scrs)\n",
    "    ]\n",
    "    #Sort on meaning, then diversity\n",
    "    top_n_paraphrases = sorted(scored_paraphrases, key=lambda x: (x[1], x[2]), reverse=True)[:top_n]\n",
    "    return [x[0] for x in top_n_paraphrases]\n",
    "\n",
    "top_para = get_n_best_para(INPUT_SEN, paraphrases, TOP_TO_KEEP)\n",
    "\n",
    "print(\"INPUT SENTENCE :\", INPUT_SEN)\n",
    "print(\"PARAPRHASES :\")\n",
    "for i, paraphrase in enumerate(top_para):\n",
    "        print(\"n°%d : %s\" % (i, paraphrase))\n",
    "\n",
    "\n",
    "print(\"INPUT SENTENCE :\", INPUT_SEN)\n",
    "print(\"PARAPRHASES :\")\n",
    "for i, paraphrase in enumerate(paraphrases):\n",
    "        print(\"n°%d : %s\" % (i, paraphrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para",
   "language": "python",
   "name": "para"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
